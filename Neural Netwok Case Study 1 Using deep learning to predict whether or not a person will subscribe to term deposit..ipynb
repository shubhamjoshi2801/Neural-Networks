{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective : \n",
    "The objective of this case study is to demonstrate how Artificial Neural Networks can be leveraged for classsification and how \n",
    "tuning an Artificial Neural Network can lead to an improvement in accuracy of predictions on either class.\n",
    "\n",
    "#### Approach :\n",
    "The dataset we have is infested with a high degreee of class imbalance. There are 39922 instaces that belong to classs 'no' while there are 5289 instances that belong to class 'yes'. Training the classification model on a dataset composed of samples drawn from such a dataset is likely to predict the classes belonging to the abundant class with greater correctness in comparision with oinstances that belong to the scarce class. We thus train the classification model on a dataset obtained by drawing an equal number of samples of either class and evaluating the performance of the model thus obtained on the remaining instances of the original dataset. In this way we can ensure that the performance of the model is balances on either classes i.e specificity and sensitivity values are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the Dataset :\n",
    "\n",
    "Data Set Information :\n",
    "DATA SOURCE : https://archive.ics.uci.edu/ml/machine-learning-databases/00222/\n",
    "\n",
    "The dataset that we have has been derived from a marketing campaign run by a Portugese Banking Institution between 2008 and 2013. By training a classifier on the dataset we have, we want to evolve a model that can be used to asses the likelihood of a client subscribing to term deposit when contacted over the telephone. Clients which have a high likelihood of subscribing to the term deposit are accorded 1 and those having low likelihood are accorded 0 by the classifier.\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "#### Input Features :\n",
    "Bank Client data:\n",
    "\n",
    "1 - age (numeric)\n",
    "\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "Related with the last contact of the current campaign :\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "Other attributes/input features :\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "Social and Economic Context attributes/input features :\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "#### Output Feature / Target Feature :\n",
    "\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Importing the relevant Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Loading the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      "age          45211 non-null int64\n",
      "job          45211 non-null object\n",
      "marital      45211 non-null object\n",
      "education    45211 non-null object\n",
      "default      45211 non-null object\n",
      "balance      45211 non-null int64\n",
      "housing      45211 non-null object\n",
      "loan         45211 non-null object\n",
      "contact      45211 non-null object\n",
      "day          45211 non-null int64\n",
      "month        45211 non-null object\n",
      "duration     45211 non-null int64\n",
      "campaign     45211 non-null int64\n",
      "pdays        45211 non-null int64\n",
      "previous     45211 non-null int64\n",
      "poutcome     45211 non-null object\n",
      "y            45211 non-null object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "bank_data=pd.read_csv('bank-full.csv',sep=';')\n",
    "bank_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     39922\n",
       "yes     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for class imbalance:\n",
    "bank_data['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fetching the categorical attributes and their corresponding numerical indices from among the categorical input features \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 7, 8, 10, 15]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe of categorical input features\n",
    "categorical_dataframe=bank_data.loc[:,'age':'poutcome'].select_dtypes(include=object)\n",
    "#extracting the features/columns of the categorical dataframe\n",
    "categorical_attributes=categorical_dataframe.columns.tolist()\n",
    "#extracting the correspoding indices of categorical features\n",
    "categorical_indices=[]\n",
    "for attribute in categorical_attributes:\n",
    "    categorical_indices.append(bank_data.columns.get_loc(attribute))\n",
    "categorical_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Label Encoding the categorical features :\n",
    "Label Encoding is the process of assigning numerical labels to values contained within categorical input features of the dataframe. Label Encoding is performed inorder to facilitate the application of predictive mathematical models such as Logistic Regression, Support Vector Machines, Naive-Baye's etc, to those datasets which contain categorical/non-numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the categorical attributes:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder_object=LabelEncoder()\n",
    "for attribute in categorical_attributes:\n",
    "    bank_data.loc[:,attribute]=encoder_object.fit_transform(bank_data.loc[:,attribute])\n",
    "bank_data.head()\n",
    "\n",
    "#label encoding the target feature:\n",
    "bank_data.loc[:,'y']=encoder_object.fit_transform(bank_data.loc[:,'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) OneHotEncoding the categorical features :\n",
    "In order to facilitate the application of mathamatical models to datasets, merely assigning numerical labels to categorical attributes is simply not enough. One must remember that the assigned numerical labels are not related to each other in an ordinal sense, therefore we use a technique called 'OneHotEncoding' which, what basically does is, the following :\n",
    "\n",
    "A column representing a categorical attribute is split into multiple columns such that we have new columns equal to the number of all the numerical labels used for encoding the values contained within the column under consideration. Inorder to expand upon what has just been stated, consider the following, the column of the dataframe named 'job' contains 41118 values, these 41118 values have been assigned numerical labels using integers from 0 to 12 i.e 13 integers. We will now split the 'job' column into 13 columns and each of the columns will represent an integer from 0 to 12.\n",
    "\n",
    "For a particular observation (row index) if the job is encoded with a label '3', it will reflect in the newly created columns in the following way, the column that reprsents label '3' will be assigned 1 whereas rest of the columns will be assigned '0' and so on. This holds true for all the encoded categorical columns.\n",
    "\n",
    "To sum up 'OneHotEncoding' can be described as the process of assigning a binary sequence of a particular 'length' to each value conatined within a 'LabelEncoded' attribute. The 'length'of the binary sequence is equal to the number of numerical labels used to represent the different values contained within a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotencoding:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#while instantiating an object of the OneHotEncoder class we pass the indices of categorical features\n",
    "encoder_object2=OneHotEncoder(categorical_features=categorical_indices)\n",
    "bank_data=encoder_object2.fit_transform(bank_data).toarray()\n",
    "bank_data=pd.DataFrame(data=bank_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Checking the number of splits rendered to each categorical attribute/feature :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>indices</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marital</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>default</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>housing</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loan</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>contact</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>month</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poutcome</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  indices  splits\n",
       "0        job        1      12\n",
       "1    marital        2       3\n",
       "2  education        3       4\n",
       "3    default        4       2\n",
       "4    housing        6       2\n",
       "5       loan        7       2\n",
       "6    contact        8       3\n",
       "7      month       10      12\n",
       "8   poutcome       15       4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df=pd.DataFrame(data={'feature':categorical_attributes,'indices':categorical_indices,'splits':encoder_object2.n_values_})\n",
    "split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Checking the transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9  ...    42   43    44  \\\n",
       "0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  1.0  58.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0 ...   0.0  1.0  44.0   \n",
       "2  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  1.0  33.0   \n",
       "3  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  1.0  47.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  1.0  33.0   \n",
       "\n",
       "       45   46     47   48   49   50   51  \n",
       "0  2143.0  5.0  261.0  1.0 -1.0  0.0  0.0  \n",
       "1    29.0  5.0  151.0  1.0 -1.0  0.0  0.0  \n",
       "2     2.0  5.0   76.0  1.0 -1.0  0.0  0.0  \n",
       "3  1506.0  5.0   92.0  1.0 -1.0  0.0  0.0  \n",
       "4     1.0  5.0  198.0  1.0 -1.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data=bank_data[bank_data[51]==1].sample(n=3000,replace=False)\n",
    "negative_data=bank_data[bank_data[51]==0].sample(n=3000,replace=False)\n",
    "training_data=pd.concat([positive_data,negative_data])\n",
    "training_data=training_data.reindex(np.random.permutation(training_data.index))\n",
    "\n",
    "#removing the indices from the main dataset:\n",
    "testing_data=bank_data.drop(training_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer=StandardScaler()\n",
    "training_data.iloc[:,0:51]=standardizer.fit_transform(training_data.iloc[:,0:51])\n",
    "testing_data.iloc[:,0:51]=standardizer.transform(testing_data.iloc[:,0:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components : 30\n"
     ]
    }
   ],
   "source": [
    "#applying pca:\n",
    "from sklearn.decomposition import PCA\n",
    "pca_object=PCA(0.85)\n",
    "\n",
    "training_input=training_data.iloc[:,0:51]\n",
    "testing_input=testing_data.iloc[:,0:51]\n",
    "\n",
    "training_input=pca_object.fit_transform(training_input)\n",
    "testing_input=pca_object.transform(testing_input)\n",
    "\n",
    "print('number of components :',pca_object.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(data=training_input)\n",
    "X_test=pd.DataFrame(data=testing_input)\n",
    "Y_train=training_data[51].values\n",
    "Y_test=testing_data[51].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241144</td>\n",
       "      <td>-1.019349</td>\n",
       "      <td>0.488315</td>\n",
       "      <td>0.776239</td>\n",
       "      <td>-0.965468</td>\n",
       "      <td>-0.053328</td>\n",
       "      <td>0.207850</td>\n",
       "      <td>-1.665873</td>\n",
       "      <td>-0.510022</td>\n",
       "      <td>-2.052039</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155946</td>\n",
       "      <td>1.319015</td>\n",
       "      <td>-1.581708</td>\n",
       "      <td>0.096150</td>\n",
       "      <td>-3.584529</td>\n",
       "      <td>2.028250</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.812986</td>\n",
       "      <td>0.469430</td>\n",
       "      <td>-1.714143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.190469</td>\n",
       "      <td>1.520604</td>\n",
       "      <td>2.408239</td>\n",
       "      <td>-1.550920</td>\n",
       "      <td>0.121868</td>\n",
       "      <td>1.177971</td>\n",
       "      <td>-0.913854</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>0.449382</td>\n",
       "      <td>2.893597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948987</td>\n",
       "      <td>-0.492823</td>\n",
       "      <td>0.842918</td>\n",
       "      <td>1.625071</td>\n",
       "      <td>2.487433</td>\n",
       "      <td>-1.528100</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>2.649644</td>\n",
       "      <td>-1.789326</td>\n",
       "      <td>-1.078712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.272051</td>\n",
       "      <td>0.055051</td>\n",
       "      <td>-0.993262</td>\n",
       "      <td>2.053624</td>\n",
       "      <td>3.310269</td>\n",
       "      <td>-1.480998</td>\n",
       "      <td>-1.864767</td>\n",
       "      <td>-0.983613</td>\n",
       "      <td>-0.660432</td>\n",
       "      <td>-1.994892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043002</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>-1.395787</td>\n",
       "      <td>0.287606</td>\n",
       "      <td>0.657211</td>\n",
       "      <td>0.055290</td>\n",
       "      <td>1.205400</td>\n",
       "      <td>-0.549031</td>\n",
       "      <td>-0.796887</td>\n",
       "      <td>-0.166154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.050385</td>\n",
       "      <td>-1.090903</td>\n",
       "      <td>-0.570913</td>\n",
       "      <td>0.149118</td>\n",
       "      <td>2.553924</td>\n",
       "      <td>-1.303054</td>\n",
       "      <td>1.191041</td>\n",
       "      <td>0.066234</td>\n",
       "      <td>0.602685</td>\n",
       "      <td>0.346460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114949</td>\n",
       "      <td>0.493352</td>\n",
       "      <td>-0.360920</td>\n",
       "      <td>-0.456799</td>\n",
       "      <td>-0.015792</td>\n",
       "      <td>-0.566415</td>\n",
       "      <td>-0.579519</td>\n",
       "      <td>-0.038638</td>\n",
       "      <td>-0.240358</td>\n",
       "      <td>0.868097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.122800</td>\n",
       "      <td>-0.470365</td>\n",
       "      <td>-2.671357</td>\n",
       "      <td>-1.641579</td>\n",
       "      <td>1.518754</td>\n",
       "      <td>0.187383</td>\n",
       "      <td>-0.720655</td>\n",
       "      <td>-0.584875</td>\n",
       "      <td>-0.116725</td>\n",
       "      <td>-0.999610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472270</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>-0.130358</td>\n",
       "      <td>-0.407093</td>\n",
       "      <td>0.109686</td>\n",
       "      <td>-0.479669</td>\n",
       "      <td>-0.624707</td>\n",
       "      <td>-0.691552</td>\n",
       "      <td>-0.142707</td>\n",
       "      <td>0.421132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.241144 -1.019349  0.488315  0.776239 -0.965468 -0.053328  0.207850   \n",
       "1  2.190469  1.520604  2.408239 -1.550920  0.121868  1.177971 -0.913854   \n",
       "2 -0.272051  0.055051 -0.993262  2.053624  3.310269 -1.480998 -1.864767   \n",
       "3 -0.050385 -1.090903 -0.570913  0.149118  2.553924 -1.303054  1.191041   \n",
       "4 -0.122800 -0.470365 -2.671357 -1.641579  1.518754  0.187383 -0.720655   \n",
       "\n",
       "         7         8         9     ...           20        21        22  \\\n",
       "0 -1.665873 -0.510022 -2.052039    ...     1.155946  1.319015 -1.581708   \n",
       "1  0.881517  0.449382  2.893597    ...    -0.948987 -0.492823  0.842918   \n",
       "2 -0.983613 -0.660432 -1.994892    ...    -0.043002  0.145348 -1.395787   \n",
       "3  0.066234  0.602685  0.346460    ...     0.114949  0.493352 -0.360920   \n",
       "4 -0.584875 -0.116725 -0.999610    ...    -0.472270  0.678100 -0.130358   \n",
       "\n",
       "         23        24        25        26        27        28        29  \n",
       "0  0.096150 -3.584529  2.028250  0.999541  0.812986  0.469430 -1.714143  \n",
       "1  1.625071  2.487433 -1.528100  0.063520  2.649644 -1.789326 -1.078712  \n",
       "2  0.287606  0.657211  0.055290  1.205400 -0.549031 -0.796887 -0.166154  \n",
       "3 -0.456799 -0.015792 -0.566415 -0.579519 -0.038638 -0.240358  0.868097  \n",
       "4 -0.407093  0.109686 -0.479669 -0.624707 -0.691552 -0.142707  0.421132  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30619  6303]\n",
      " [  342  1947]]\n",
      "83.05322486037082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf=SVC()\n",
    "svm_clf.fit(X_train,Y_train)\n",
    "Y_pred=svm_clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(100*accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the ANN:\n",
    "classifier=Sequential() #instantiating an object of the sequential class\n",
    "input_layer=Dense(units=40,kernel_initializer='uniform',activation='relu',input_dim=30) #adding hidden layers\n",
    "classifier.add(input_layer)\n",
    "for count in range(3):#adding hidden layers\n",
    "    hidden_layer=Dense(units=40,kernel_initializer='uniform',activation='relu')\n",
    "    classifier.add(hidden_layer)\n",
    "    classifier.add(Dropout(rate=0.15))\n",
    "output_layer=Dense(units=1,kernel_initializer='uniform',activation='sigmoid')# adding output layer\n",
    "classifier.add(output_layer)\n",
    "classifier.layers\n",
    "\n",
    "#compiling the neural network:\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 0s 62us/step - loss: 0.6678 - acc: 0.6633\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.4248 - acc: 0.8192\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.3907 - acc: 0.8370\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3733 - acc: 0.8422\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.3663 - acc: 0.8497\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.3582 - acc: 0.8547\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3491 - acc: 0.8592\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3434 - acc: 0.8610\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3395 - acc: 0.8627\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3330 - acc: 0.8682\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3268 - acc: 0.8690\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3248 - acc: 0.8727\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3220 - acc: 0.8710\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3170 - acc: 0.8733\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3157 - acc: 0.8765\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3092 - acc: 0.8788\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3092 - acc: 0.8763\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8810\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3015 - acc: 0.8838\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8838\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2970 - acc: 0.8852\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2956 - acc: 0.8855\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.8855\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2858 - acc: 0.8888\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2886 - acc: 0.8845\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2852 - acc: 0.8890\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2845 - acc: 0.8880\n",
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2752 - acc: 0.8915\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2805 - acc: 0.8892\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2772 - acc: 0.8905\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2667 - acc: 0.8953\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2722 - acc: 0.8972\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2713 - acc: 0.8973\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2675 - acc: 0.8982\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2656 - acc: 0.8993\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2612 - acc: 0.8988\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2664 - acc: 0.9005\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2593 - acc: 0.8993\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2617 - acc: 0.8993\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2536 - acc: 0.9002\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2510 - acc: 0.9028\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2560 - acc: 0.9017\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2543 - acc: 0.9025\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2449 - acc: 0.9055\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2509 - acc: 0.9022\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2520 - acc: 0.9038\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 0s 17us/step - loss: 0.2453 - acc: 0.9043\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 0s 17us/step - loss: 0.2458 - acc: 0.9040\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 0s 17us/step - loss: 0.2465 - acc: 0.9048\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 0s 17us/step - loss: 0.2466 - acc: 0.9043\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2408 - acc: 0.9102\n",
      "Epoch 52/100\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.2430 - acc: 0.905 - 0s 15us/step - loss: 0.2387 - acc: 0.9085\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2372 - acc: 0.9065\n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2353 - acc: 0.9112\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2399 - acc: 0.9085\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2339 - acc: 0.9103\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 0s 17us/step - loss: 0.2244 - acc: 0.9160\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 0s 16us/step - loss: 0.2284 - acc: 0.9115\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2297 - acc: 0.9097\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2248 - acc: 0.9162\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2225 - acc: 0.9130\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2282 - acc: 0.9135\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2196 - acc: 0.9155\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2211 - acc: 0.9145\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2203 - acc: 0.9172\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2224 - acc: 0.9167\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2165 - acc: 0.9163\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2198 - acc: 0.9140\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2072 - acc: 0.9177\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2097 - acc: 0.9210\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2102 - acc: 0.9188\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 0s 14us/step - loss: 0.2089 - acc: 0.9223\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2134 - acc: 0.9173\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2040 - acc: 0.9220\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2064 - acc: 0.9205: 0s - loss: 0.2041 - acc: 0.921\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2030 - acc: 0.9243\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2038 - acc: 0.9222\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1991 - acc: 0.9223\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2042 - acc: 0.9223\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2125 - acc: 0.9157\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2030 - acc: 0.9223\n",
      "Epoch 82/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2008 - acc: 0.9232\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 0s 14us/step - loss: 0.2017 - acc: 0.9215\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1948 - acc: 0.9252\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1966 - acc: 0.9265\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 0s 14us/step - loss: 0.1879 - acc: 0.9302\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1955 - acc: 0.9240\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1934 - acc: 0.924 - 0s 15us/step - loss: 0.1970 - acc: 0.9258\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.2000 - acc: 0.9240\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1914 - acc: 0.9253\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1908 - acc: 0.9243\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 0s 14us/step - loss: 0.1843 - acc: 0.9272\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1840 - acc: 0.9268\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1840 - acc: 0.9310\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1953 - acc: 0.9260\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1807 - acc: 0.9302\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1808 - acc: 0.9307\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1907 - acc: 0.9247\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1993 - acc: 0.9255\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 0s 15us/step - loss: 0.1793 - acc: 0.9317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ccb572748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " classifier.fit(X_train,Y_train,batch_size=100,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "Y_pred=classifier.predict(X_test)\n",
    "Y_pred = binarize(Y_pred,threshold=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30295  6627]\n",
      " [  335  1954]]\n",
      "82.244778251001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(100*accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4800/4800 [==============================] - 0s 70us/step - loss: 0.5924 - acc: 0.7329\n",
      "Epoch 2/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3946 - acc: 0.8346\n",
      "Epoch 3/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3763 - acc: 0.8427\n",
      "Epoch 4/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3673 - acc: 0.8477\n",
      "Epoch 5/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3553 - acc: 0.8565\n",
      "Epoch 6/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3467 - acc: 0.8548\n",
      "Epoch 7/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3407 - acc: 0.8602\n",
      "Epoch 8/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3341 - acc: 0.8642\n",
      "Epoch 9/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3280 - acc: 0.8677\n",
      "Epoch 10/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3219 - acc: 0.8690\n",
      "Epoch 11/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3205 - acc: 0.8690\n",
      "Epoch 12/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3140 - acc: 0.8748\n",
      "Epoch 13/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3129 - acc: 0.8742\n",
      "Epoch 14/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3076 - acc: 0.8771\n",
      "Epoch 15/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2993 - acc: 0.8806: 0s - loss: 0.2903 - acc: 0.883\n",
      "Epoch 16/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3023 - acc: 0.8781\n",
      "Epoch 17/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2925 - acc: 0.8842\n",
      "Epoch 18/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2877 - acc: 0.8865\n",
      "Epoch 19/100\n",
      "4800/4800 [==============================] - ETA: 0s - loss: 0.2885 - acc: 0.888 - 0s 19us/step - loss: 0.2857 - acc: 0.8910\n",
      "Epoch 20/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2838 - acc: 0.8917\n",
      "Epoch 21/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2811 - acc: 0.8908\n",
      "Epoch 22/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2787 - acc: 0.8900\n",
      "Epoch 23/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2795 - acc: 0.8896\n",
      "Epoch 24/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2703 - acc: 0.8940\n",
      "Epoch 25/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2736 - acc: 0.8952\n",
      "Epoch 26/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2722 - acc: 0.8946\n",
      "Epoch 27/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2646 - acc: 0.8979\n",
      "Epoch 28/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2638 - acc: 0.8985\n",
      "Epoch 29/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2612 - acc: 0.8971: 0s - loss: 0.2608 - acc: 0.898\n",
      "Epoch 30/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2619 - acc: 0.8937\n",
      "Epoch 31/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2570 - acc: 0.8998\n",
      "Epoch 32/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2613 - acc: 0.8944\n",
      "Epoch 33/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2532 - acc: 0.8990\n",
      "Epoch 34/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2551 - acc: 0.8990\n",
      "Epoch 35/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2548 - acc: 0.9008\n",
      "Epoch 36/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2527 - acc: 0.9029\n",
      "Epoch 37/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2479 - acc: 0.9017\n",
      "Epoch 38/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2479 - acc: 0.9023\n",
      "Epoch 39/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2471 - acc: 0.9012\n",
      "Epoch 40/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2398 - acc: 0.9073\n",
      "Epoch 41/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2388 - acc: 0.9050\n",
      "Epoch 42/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2411 - acc: 0.9054\n",
      "Epoch 43/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2378 - acc: 0.9042\n",
      "Epoch 44/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2346 - acc: 0.9075\n",
      "Epoch 45/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2312 - acc: 0.9079\n",
      "Epoch 46/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2313 - acc: 0.9067\n",
      "Epoch 47/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2304 - acc: 0.9079\n",
      "Epoch 48/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2291 - acc: 0.9096\n",
      "Epoch 49/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2257 - acc: 0.9102\n",
      "Epoch 50/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2253 - acc: 0.9123\n",
      "Epoch 51/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2219 - acc: 0.9106\n",
      "Epoch 52/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2177 - acc: 0.9131\n",
      "Epoch 53/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2191 - acc: 0.9096\n",
      "Epoch 54/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2191 - acc: 0.9104\n",
      "Epoch 55/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2151 - acc: 0.9148\n",
      "Epoch 56/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2168 - acc: 0.9125\n",
      "Epoch 57/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2118 - acc: 0.9142\n",
      "Epoch 58/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2093 - acc: 0.9108\n",
      "Epoch 59/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2123 - acc: 0.9123\n",
      "Epoch 60/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2047 - acc: 0.9148\n",
      "Epoch 61/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2063 - acc: 0.9169\n",
      "Epoch 62/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2058 - acc: 0.9121\n",
      "Epoch 63/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1978 - acc: 0.9227\n",
      "Epoch 64/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1985 - acc: 0.9235\n",
      "Epoch 65/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1991 - acc: 0.9196\n",
      "Epoch 66/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1967 - acc: 0.9200\n",
      "Epoch 67/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1998 - acc: 0.9179\n",
      "Epoch 68/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1916 - acc: 0.9229: 0s - loss: 0.1801 - acc: 0.927\n",
      "Epoch 69/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1964 - acc: 0.9254\n",
      "Epoch 70/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1894 - acc: 0.9227\n",
      "Epoch 71/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1872 - acc: 0.9221\n",
      "Epoch 72/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1935 - acc: 0.9219\n",
      "Epoch 73/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1911 - acc: 0.9219\n",
      "Epoch 74/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1778 - acc: 0.9281\n",
      "Epoch 75/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1845 - acc: 0.9258\n",
      "Epoch 76/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1870 - acc: 0.9242\n",
      "Epoch 77/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1828 - acc: 0.9248\n",
      "Epoch 78/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1848 - acc: 0.9254\n",
      "Epoch 79/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1781 - acc: 0.9312\n",
      "Epoch 80/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1734 - acc: 0.9310\n",
      "Epoch 81/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1771 - acc: 0.9296\n",
      "Epoch 82/100\n",
      "4800/4800 [==============================] - 0s 16us/step - loss: 0.1781 - acc: 0.9287\n",
      "Epoch 83/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1856 - acc: 0.9262\n",
      "Epoch 84/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1763 - acc: 0.9308\n",
      "Epoch 85/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1652 - acc: 0.9356\n",
      "Epoch 86/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1629 - acc: 0.9356\n",
      "Epoch 87/100\n",
      "4800/4800 [==============================] - ETA: 0s - loss: 0.1549 - acc: 0.938 - 0s 17us/step - loss: 0.1667 - acc: 0.9327\n",
      "Epoch 88/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1749 - acc: 0.9308\n",
      "Epoch 89/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1839 - acc: 0.9260\n",
      "Epoch 90/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1622 - acc: 0.9356\n",
      "Epoch 91/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1601 - acc: 0.9350\n",
      "Epoch 92/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1676 - acc: 0.9329\n",
      "Epoch 93/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1600 - acc: 0.9360\n",
      "Epoch 94/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1642 - acc: 0.9327\n",
      "Epoch 95/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1602 - acc: 0.9352\n",
      "Epoch 96/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1571 - acc: 0.9362\n",
      "Epoch 97/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1534 - acc: 0.9381\n",
      "Epoch 98/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1585 - acc: 0.9377\n",
      "Epoch 99/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1620 - acc: 0.9335\n",
      "Epoch 100/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1503 - acc: 0.9352\n",
      "1200/1200 [==============================] - 0s 45us/step\n",
      "Epoch 1/100\n",
      "4800/4800 [==============================] - 0s 77us/step - loss: 0.6073 - acc: 0.6948\n",
      "Epoch 2/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.4012 - acc: 0.8304\n",
      "Epoch 3/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3773 - acc: 0.8419\n",
      "Epoch 4/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3641 - acc: 0.8440\n",
      "Epoch 5/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3494 - acc: 0.8546\n",
      "Epoch 6/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3440 - acc: 0.8569\n",
      "Epoch 7/100\n",
      "4800/4800 [==============================] - ETA: 0s - loss: 0.3517 - acc: 0.855 - 0s 20us/step - loss: 0.3361 - acc: 0.8650\n",
      "Epoch 8/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3320 - acc: 0.8610\n",
      "Epoch 9/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3259 - acc: 0.8654\n",
      "Epoch 10/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3164 - acc: 0.8721\n",
      "Epoch 11/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3149 - acc: 0.8723\n",
      "Epoch 12/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3105 - acc: 0.8729\n",
      "Epoch 13/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3059 - acc: 0.8760\n",
      "Epoch 14/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2988 - acc: 0.8812\n",
      "Epoch 15/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2957 - acc: 0.8831\n",
      "Epoch 16/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2973 - acc: 0.8812\n",
      "Epoch 17/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2873 - acc: 0.8869\n",
      "Epoch 18/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2893 - acc: 0.8831\n",
      "Epoch 19/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2853 - acc: 0.8883\n",
      "Epoch 20/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2795 - acc: 0.8917\n",
      "Epoch 21/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2769 - acc: 0.8948\n",
      "Epoch 22/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2791 - acc: 0.8892\n",
      "Epoch 23/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2751 - acc: 0.8908\n",
      "Epoch 24/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2709 - acc: 0.8960\n",
      "Epoch 25/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2639 - acc: 0.8990\n",
      "Epoch 26/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2658 - acc: 0.8950\n",
      "Epoch 27/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2693 - acc: 0.8925\n",
      "Epoch 28/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2601 - acc: 0.9025\n",
      "Epoch 29/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2584 - acc: 0.9004\n",
      "Epoch 30/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2579 - acc: 0.8967\n",
      "Epoch 31/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2603 - acc: 0.9010\n",
      "Epoch 32/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2556 - acc: 0.9025\n",
      "Epoch 33/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2510 - acc: 0.9023\n",
      "Epoch 34/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2505 - acc: 0.9042\n",
      "Epoch 35/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2464 - acc: 0.9062\n",
      "Epoch 36/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2463 - acc: 0.9075\n",
      "Epoch 37/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2445 - acc: 0.9054\n",
      "Epoch 38/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2419 - acc: 0.9081\n",
      "Epoch 39/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2419 - acc: 0.9098\n",
      "Epoch 40/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2390 - acc: 0.9077\n",
      "Epoch 41/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2392 - acc: 0.9075\n",
      "Epoch 42/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2380 - acc: 0.9119\n",
      "Epoch 43/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2338 - acc: 0.9144\n",
      "Epoch 44/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2326 - acc: 0.9106\n",
      "Epoch 45/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2261 - acc: 0.9148\n",
      "Epoch 46/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2257 - acc: 0.9123\n",
      "Epoch 47/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2253 - acc: 0.9137\n",
      "Epoch 48/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2215 - acc: 0.9169\n",
      "Epoch 49/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2216 - acc: 0.9171\n",
      "Epoch 50/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2172 - acc: 0.9175\n",
      "Epoch 51/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2164 - acc: 0.9194\n",
      "Epoch 52/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2193 - acc: 0.9173\n",
      "Epoch 53/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2171 - acc: 0.9177\n",
      "Epoch 54/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2082 - acc: 0.9219\n",
      "Epoch 55/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2137 - acc: 0.9181\n",
      "Epoch 56/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2097 - acc: 0.9215\n",
      "Epoch 57/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2098 - acc: 0.9212\n",
      "Epoch 58/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2063 - acc: 0.9227\n",
      "Epoch 59/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2037 - acc: 0.9235\n",
      "Epoch 60/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2017 - acc: 0.9225\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2029 - acc: 0.9240\n",
      "Epoch 62/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2004 - acc: 0.9248\n",
      "Epoch 63/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1973 - acc: 0.9271\n",
      "Epoch 64/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1946 - acc: 0.9279\n",
      "Epoch 65/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1935 - acc: 0.9258\n",
      "Epoch 66/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1893 - acc: 0.9302\n",
      "Epoch 67/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1884 - acc: 0.9300\n",
      "Epoch 68/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1843 - acc: 0.9296\n",
      "Epoch 69/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1883 - acc: 0.9308\n",
      "Epoch 70/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1938 - acc: 0.9262\n",
      "Epoch 71/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1843 - acc: 0.9317\n",
      "Epoch 72/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1828 - acc: 0.9312\n",
      "Epoch 73/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1825 - acc: 0.9331\n",
      "Epoch 74/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1889 - acc: 0.9254\n",
      "Epoch 75/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1778 - acc: 0.9323\n",
      "Epoch 76/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1797 - acc: 0.9315\n",
      "Epoch 77/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1749 - acc: 0.9300\n",
      "Epoch 78/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1764 - acc: 0.9300\n",
      "Epoch 79/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1670 - acc: 0.9352\n",
      "Epoch 80/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1683 - acc: 0.9356\n",
      "Epoch 81/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1711 - acc: 0.9348\n",
      "Epoch 82/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1807 - acc: 0.9302\n",
      "Epoch 83/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1632 - acc: 0.9369\n",
      "Epoch 84/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1666 - acc: 0.9367\n",
      "Epoch 85/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1658 - acc: 0.9346\n",
      "Epoch 86/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1752 - acc: 0.9296\n",
      "Epoch 87/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1576 - acc: 0.9379\n",
      "Epoch 88/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1549 - acc: 0.9423\n",
      "Epoch 89/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1557 - acc: 0.9410\n",
      "Epoch 90/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1621 - acc: 0.9356\n",
      "Epoch 91/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1651 - acc: 0.9346\n",
      "Epoch 92/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1641 - acc: 0.9360\n",
      "Epoch 93/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1515 - acc: 0.9390\n",
      "Epoch 94/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1526 - acc: 0.9394\n",
      "Epoch 95/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1417 - acc: 0.9467\n",
      "Epoch 96/100\n",
      "4800/4800 [==============================] - ETA: 0s - loss: 0.1234 - acc: 0.953 - 0s 20us/step - loss: 0.1352 - acc: 0.9479\n",
      "Epoch 97/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1504 - acc: 0.9398\n",
      "Epoch 98/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1534 - acc: 0.9373\n",
      "Epoch 99/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1508 - acc: 0.9400\n",
      "Epoch 100/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1535 - acc: 0.9398\n",
      "1200/1200 [==============================] - 0s 57us/step\n",
      "Epoch 1/100\n",
      "4800/4800 [==============================] - 0s 82us/step - loss: 0.6135 - acc: 0.6725\n",
      "Epoch 2/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3995 - acc: 0.8327\n",
      "Epoch 3/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3742 - acc: 0.8442\n",
      "Epoch 4/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3587 - acc: 0.8485\n",
      "Epoch 5/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3485 - acc: 0.8558\n",
      "Epoch 6/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3409 - acc: 0.8594\n",
      "Epoch 7/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3281 - acc: 0.8658\n",
      "Epoch 8/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3242 - acc: 0.8660\n",
      "Epoch 9/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3190 - acc: 0.8717\n",
      "Epoch 10/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3088 - acc: 0.8779\n",
      "Epoch 11/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.3057 - acc: 0.8758\n",
      "Epoch 12/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3007 - acc: 0.8783\n",
      "Epoch 13/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2990 - acc: 0.8810\n",
      "Epoch 14/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2895 - acc: 0.8856\n",
      "Epoch 15/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2882 - acc: 0.8887\n",
      "Epoch 16/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2872 - acc: 0.8871\n",
      "Epoch 17/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2841 - acc: 0.8908\n",
      "Epoch 18/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2746 - acc: 0.8956\n",
      "Epoch 19/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2762 - acc: 0.8990\n",
      "Epoch 20/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2726 - acc: 0.8910\n",
      "Epoch 21/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2735 - acc: 0.8931\n",
      "Epoch 22/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2684 - acc: 0.8962\n",
      "Epoch 23/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2618 - acc: 0.9010\n",
      "Epoch 24/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2646 - acc: 0.8967\n",
      "Epoch 25/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2590 - acc: 0.8971\n",
      "Epoch 26/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2554 - acc: 0.9042\n",
      "Epoch 27/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2576 - acc: 0.9017\n",
      "Epoch 28/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2561 - acc: 0.9008\n",
      "Epoch 29/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2503 - acc: 0.9054\n",
      "Epoch 30/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2475 - acc: 0.9037\n",
      "Epoch 31/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2479 - acc: 0.9073\n",
      "Epoch 32/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2442 - acc: 0.9052\n",
      "Epoch 33/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2390 - acc: 0.9096\n",
      "Epoch 34/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2457 - acc: 0.9042: 0s - loss: 0.2400 - acc: 0.906\n",
      "Epoch 35/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2362 - acc: 0.9094\n",
      "Epoch 36/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2354 - acc: 0.9125\n",
      "Epoch 37/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2337 - acc: 0.9115\n",
      "Epoch 38/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2317 - acc: 0.9135\n",
      "Epoch 39/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2315 - acc: 0.9098\n",
      "Epoch 40/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2285 - acc: 0.9127\n",
      "Epoch 41/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2222 - acc: 0.9177\n",
      "Epoch 42/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2270 - acc: 0.9119\n",
      "Epoch 43/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2162 - acc: 0.9183\n",
      "Epoch 44/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2168 - acc: 0.9185\n",
      "Epoch 45/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2141 - acc: 0.9190\n",
      "Epoch 46/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2175 - acc: 0.9181\n",
      "Epoch 47/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2140 - acc: 0.9202\n",
      "Epoch 48/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2094 - acc: 0.9210\n",
      "Epoch 49/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2079 - acc: 0.9221\n",
      "Epoch 50/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2125 - acc: 0.9181\n",
      "Epoch 51/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2038 - acc: 0.9215\n",
      "Epoch 52/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2018 - acc: 0.9231\n",
      "Epoch 53/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2066 - acc: 0.9204\n",
      "Epoch 54/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1971 - acc: 0.9246\n",
      "Epoch 55/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1954 - acc: 0.9271\n",
      "Epoch 56/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.2000 - acc: 0.9240\n",
      "Epoch 57/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1991 - acc: 0.9229\n",
      "Epoch 58/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1911 - acc: 0.9269\n",
      "Epoch 59/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1885 - acc: 0.9254\n",
      "Epoch 60/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1889 - acc: 0.9254\n",
      "Epoch 61/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1892 - acc: 0.9263\n",
      "Epoch 62/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1852 - acc: 0.9304\n",
      "Epoch 63/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1825 - acc: 0.9279\n",
      "Epoch 64/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1935 - acc: 0.9221\n",
      "Epoch 65/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1735 - acc: 0.9344\n",
      "Epoch 66/100\n",
      "4800/4800 [==============================] - 0s 17us/step - loss: 0.1741 - acc: 0.9327\n",
      "Epoch 67/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1684 - acc: 0.9377\n",
      "Epoch 68/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1716 - acc: 0.9344\n",
      "Epoch 69/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1824 - acc: 0.9315\n",
      "Epoch 70/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.1752 - acc: 0.9325\n",
      "Epoch 71/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1665 - acc: 0.9369\n",
      "Epoch 72/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1624 - acc: 0.9385\n",
      "Epoch 73/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1715 - acc: 0.9327\n",
      "Epoch 74/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1644 - acc: 0.9385\n",
      "Epoch 75/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1621 - acc: 0.9379\n",
      "Epoch 76/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1624 - acc: 0.9383\n",
      "Epoch 77/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1565 - acc: 0.9423\n",
      "Epoch 78/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1541 - acc: 0.9433\n",
      "Epoch 79/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1533 - acc: 0.9427\n",
      "Epoch 80/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1498 - acc: 0.9442\n",
      "Epoch 81/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1498 - acc: 0.9408\n",
      "Epoch 82/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1554 - acc: 0.9419\n",
      "Epoch 83/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1571 - acc: 0.9408\n",
      "Epoch 84/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1564 - acc: 0.9404\n",
      "Epoch 85/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1630 - acc: 0.9363\n",
      "Epoch 86/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1499 - acc: 0.9475\n",
      "Epoch 87/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1509 - acc: 0.9419\n",
      "Epoch 88/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1542 - acc: 0.9429\n",
      "Epoch 89/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1389 - acc: 0.9494\n",
      "Epoch 90/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1470 - acc: 0.9438\n",
      "Epoch 91/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1327 - acc: 0.9508\n",
      "Epoch 92/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1411 - acc: 0.9462\n",
      "Epoch 93/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1350 - acc: 0.9510\n",
      "Epoch 94/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1348 - acc: 0.9535\n",
      "Epoch 95/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1373 - acc: 0.9515\n",
      "Epoch 96/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1388 - acc: 0.9506\n",
      "Epoch 97/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1324 - acc: 0.9502\n",
      "Epoch 98/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1276 - acc: 0.9560\n",
      "Epoch 99/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1319 - acc: 0.9515\n",
      "Epoch 100/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1329 - acc: 0.9525\n",
      "1200/1200 [==============================] - 0s 81us/step\n",
      "Epoch 1/100\n",
      "4800/4800 [==============================] - 0s 91us/step - loss: 0.6353 - acc: 0.6306\n",
      "Epoch 2/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.4288 - acc: 0.8173\n",
      "Epoch 3/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3938 - acc: 0.8315\n",
      "Epoch 4/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.3749 - acc: 0.8421\n",
      "Epoch 5/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.3656 - acc: 0.8454\n",
      "Epoch 6/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3524 - acc: 0.8542\n",
      "Epoch 7/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3453 - acc: 0.8585\n",
      "Epoch 8/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3398 - acc: 0.8581\n",
      "Epoch 9/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.3297 - acc: 0.8646\n",
      "Epoch 10/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.3255 - acc: 0.8710\n",
      "Epoch 11/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.3182 - acc: 0.8679\n",
      "Epoch 12/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.3191 - acc: 0.8758\n",
      "Epoch 13/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.3102 - acc: 0.8769\n",
      "Epoch 14/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.3088 - acc: 0.8781\n",
      "Epoch 15/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.3034 - acc: 0.8794\n",
      "Epoch 16/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2999 - acc: 0.8792\n",
      "Epoch 17/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2968 - acc: 0.8821\n",
      "Epoch 18/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2980 - acc: 0.8827\n",
      "Epoch 19/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2948 - acc: 0.8831\n",
      "Epoch 20/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2896 - acc: 0.8890\n",
      "Epoch 21/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2864 - acc: 0.8894\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2870 - acc: 0.8887\n",
      "Epoch 23/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2829 - acc: 0.8892\n",
      "Epoch 24/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2799 - acc: 0.8921\n",
      "Epoch 25/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2758 - acc: 0.8921\n",
      "Epoch 26/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2815 - acc: 0.8890\n",
      "Epoch 27/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2723 - acc: 0.8944\n",
      "Epoch 28/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2697 - acc: 0.8937\n",
      "Epoch 29/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2663 - acc: 0.8990\n",
      "Epoch 30/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2730 - acc: 0.8933\n",
      "Epoch 31/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2644 - acc: 0.9000\n",
      "Epoch 32/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2598 - acc: 0.8996\n",
      "Epoch 33/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2584 - acc: 0.9019\n",
      "Epoch 34/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2534 - acc: 0.9040\n",
      "Epoch 35/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2540 - acc: 0.9031\n",
      "Epoch 36/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2483 - acc: 0.9040\n",
      "Epoch 37/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2508 - acc: 0.9044\n",
      "Epoch 38/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2456 - acc: 0.9083\n",
      "Epoch 39/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2434 - acc: 0.9087\n",
      "Epoch 40/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2528 - acc: 0.9050\n",
      "Epoch 41/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2404 - acc: 0.9090\n",
      "Epoch 42/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2386 - acc: 0.9098\n",
      "Epoch 43/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2372 - acc: 0.9094\n",
      "Epoch 44/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2366 - acc: 0.9100\n",
      "Epoch 45/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2324 - acc: 0.9087\n",
      "Epoch 46/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2313 - acc: 0.9115\n",
      "Epoch 47/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2271 - acc: 0.9108\n",
      "Epoch 48/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2237 - acc: 0.9131\n",
      "Epoch 49/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.2249 - acc: 0.9179\n",
      "Epoch 50/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2238 - acc: 0.9140\n",
      "Epoch 51/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2222 - acc: 0.9129\n",
      "Epoch 52/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2205 - acc: 0.9158\n",
      "Epoch 53/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2185 - acc: 0.9190\n",
      "Epoch 54/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2139 - acc: 0.9154\n",
      "Epoch 55/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2137 - acc: 0.9202\n",
      "Epoch 56/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.2142 - acc: 0.9187\n",
      "Epoch 57/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2093 - acc: 0.9219\n",
      "Epoch 58/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2099 - acc: 0.9204\n",
      "Epoch 59/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2025 - acc: 0.9265\n",
      "Epoch 60/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2047 - acc: 0.9212\n",
      "Epoch 61/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2017 - acc: 0.9219\n",
      "Epoch 62/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2010 - acc: 0.9233\n",
      "Epoch 63/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1979 - acc: 0.9262\n",
      "Epoch 64/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2082 - acc: 0.9212\n",
      "Epoch 65/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1956 - acc: 0.9244\n",
      "Epoch 66/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1988 - acc: 0.9227\n",
      "Epoch 67/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1924 - acc: 0.9275\n",
      "Epoch 68/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1892 - acc: 0.9275\n",
      "Epoch 69/100\n",
      "4800/4800 [==============================] - 0s 24us/step - loss: 0.1913 - acc: 0.9269\n",
      "Epoch 70/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1844 - acc: 0.9283\n",
      "Epoch 71/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1881 - acc: 0.9290\n",
      "Epoch 72/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1880 - acc: 0.9298\n",
      "Epoch 73/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1862 - acc: 0.9283\n",
      "Epoch 74/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1882 - acc: 0.9256\n",
      "Epoch 75/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1815 - acc: 0.9308\n",
      "Epoch 76/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1841 - acc: 0.9296\n",
      "Epoch 77/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1752 - acc: 0.9319\n",
      "Epoch 78/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.1693 - acc: 0.9342\n",
      "Epoch 79/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1686 - acc: 0.9350\n",
      "Epoch 80/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1688 - acc: 0.9354\n",
      "Epoch 81/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1774 - acc: 0.9304\n",
      "Epoch 82/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1769 - acc: 0.9300\n",
      "Epoch 83/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1730 - acc: 0.9310\n",
      "Epoch 84/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1714 - acc: 0.9348\n",
      "Epoch 85/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1690 - acc: 0.9344\n",
      "Epoch 86/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1733 - acc: 0.9315\n",
      "Epoch 87/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1655 - acc: 0.9365\n",
      "Epoch 88/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1650 - acc: 0.9327\n",
      "Epoch 89/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1708 - acc: 0.9335\n",
      "Epoch 90/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1655 - acc: 0.9335\n",
      "Epoch 91/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1614 - acc: 0.9346\n",
      "Epoch 92/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1642 - acc: 0.9346\n",
      "Epoch 93/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1665 - acc: 0.9329\n",
      "Epoch 94/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1573 - acc: 0.9369\n",
      "Epoch 95/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1570 - acc: 0.9396\n",
      "Epoch 96/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1501 - acc: 0.9381\n",
      "Epoch 97/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1530 - acc: 0.9406\n",
      "Epoch 98/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.1595 - acc: 0.9367\n",
      "Epoch 99/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1572 - acc: 0.9371\n",
      "Epoch 100/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1610 - acc: 0.9367\n",
      "1200/1200 [==============================] - 0s 79us/step\n",
      "Epoch 1/100\n",
      "4800/4800 [==============================] - 0s 91us/step - loss: 0.6202 - acc: 0.6546\n",
      "Epoch 2/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.4125 - acc: 0.8250\n",
      "Epoch 3/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3838 - acc: 0.8415\n",
      "Epoch 4/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3714 - acc: 0.8431\n",
      "Epoch 5/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3574 - acc: 0.8521\n",
      "Epoch 6/100\n",
      "4800/4800 [==============================] - 0s 18us/step - loss: 0.3485 - acc: 0.8569\n",
      "Epoch 7/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3366 - acc: 0.8635\n",
      "Epoch 8/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3291 - acc: 0.8652\n",
      "Epoch 9/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3214 - acc: 0.8694\n",
      "Epoch 10/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.3173 - acc: 0.8715\n",
      "Epoch 11/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3094 - acc: 0.8756\n",
      "Epoch 12/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3098 - acc: 0.8704\n",
      "Epoch 13/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.3011 - acc: 0.8796\n",
      "Epoch 14/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2980 - acc: 0.8837\n",
      "Epoch 15/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2907 - acc: 0.8887\n",
      "Epoch 16/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2894 - acc: 0.8862\n",
      "Epoch 17/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2843 - acc: 0.8887\n",
      "Epoch 18/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2825 - acc: 0.8906\n",
      "Epoch 19/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2848 - acc: 0.8904\n",
      "Epoch 20/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2793 - acc: 0.8923\n",
      "Epoch 21/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2831 - acc: 0.8900\n",
      "Epoch 22/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2807 - acc: 0.8904\n",
      "Epoch 23/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2687 - acc: 0.8985\n",
      "Epoch 24/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2704 - acc: 0.8950\n",
      "Epoch 25/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2675 - acc: 0.8969\n",
      "Epoch 26/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2650 - acc: 0.9002\n",
      "Epoch 27/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2635 - acc: 0.9021\n",
      "Epoch 28/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2607 - acc: 0.8979\n",
      "Epoch 29/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2611 - acc: 0.8987\n",
      "Epoch 30/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2590 - acc: 0.9002\n",
      "Epoch 31/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2576 - acc: 0.9027\n",
      "Epoch 32/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2580 - acc: 0.9025\n",
      "Epoch 33/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2534 - acc: 0.9056\n",
      "Epoch 34/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2556 - acc: 0.9008\n",
      "Epoch 35/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2519 - acc: 0.9021\n",
      "Epoch 36/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2487 - acc: 0.9083\n",
      "Epoch 37/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2425 - acc: 0.9121\n",
      "Epoch 38/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2433 - acc: 0.9102\n",
      "Epoch 39/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2432 - acc: 0.9073\n",
      "Epoch 40/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2403 - acc: 0.9106\n",
      "Epoch 41/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2378 - acc: 0.9104\n",
      "Epoch 42/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2419 - acc: 0.9079\n",
      "Epoch 43/100\n",
      "4800/4800 [==============================] - 0s 24us/step - loss: 0.2340 - acc: 0.9104\n",
      "Epoch 44/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2400 - acc: 0.9081\n",
      "Epoch 45/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2310 - acc: 0.9135\n",
      "Epoch 46/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2372 - acc: 0.9100\n",
      "Epoch 47/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2287 - acc: 0.9121\n",
      "Epoch 48/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2273 - acc: 0.9129\n",
      "Epoch 49/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2279 - acc: 0.9150\n",
      "Epoch 50/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2295 - acc: 0.9102\n",
      "Epoch 51/100\n",
      "4800/4800 [==============================] - 0s 23us/step - loss: 0.2278 - acc: 0.9152\n",
      "Epoch 52/100\n",
      "4800/4800 [==============================] - 0s 22us/step - loss: 0.2239 - acc: 0.9135\n",
      "Epoch 53/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2220 - acc: 0.9177\n",
      "Epoch 54/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2206 - acc: 0.9167\n",
      "Epoch 55/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2178 - acc: 0.9165\n",
      "Epoch 56/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2188 - acc: 0.9158\n",
      "Epoch 57/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2241 - acc: 0.9123\n",
      "Epoch 58/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2159 - acc: 0.9179\n",
      "Epoch 59/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2150 - acc: 0.9158\n",
      "Epoch 60/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2135 - acc: 0.9217\n",
      "Epoch 61/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2099 - acc: 0.9154\n",
      "Epoch 62/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2065 - acc: 0.9221\n",
      "Epoch 63/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2096 - acc: 0.9206\n",
      "Epoch 64/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2086 - acc: 0.9196\n",
      "Epoch 65/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2112 - acc: 0.9173\n",
      "Epoch 66/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2077 - acc: 0.9202\n",
      "Epoch 67/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2083 - acc: 0.9175\n",
      "Epoch 68/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2012 - acc: 0.9246\n",
      "Epoch 69/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2066 - acc: 0.9208\n",
      "Epoch 70/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2061 - acc: 0.9187\n",
      "Epoch 71/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1963 - acc: 0.9237\n",
      "Epoch 72/100\n",
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.2014 - acc: 0.9221\n",
      "Epoch 73/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.2025 - acc: 0.9194\n",
      "Epoch 74/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1981 - acc: 0.9233\n",
      "Epoch 75/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1940 - acc: 0.9258\n",
      "Epoch 76/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1922 - acc: 0.9260\n",
      "Epoch 77/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1889 - acc: 0.9279\n",
      "Epoch 78/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1894 - acc: 0.9267\n",
      "Epoch 79/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1878 - acc: 0.9275\n",
      "Epoch 80/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1960 - acc: 0.9240\n",
      "Epoch 81/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1882 - acc: 0.9250\n",
      "Epoch 82/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1843 - acc: 0.9310\n",
      "Epoch 83/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1829 - acc: 0.9294\n",
      "Epoch 84/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1818 - acc: 0.9281\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4800 [==============================] - 0s 21us/step - loss: 0.1818 - acc: 0.9308\n",
      "Epoch 86/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1837 - acc: 0.9283\n",
      "Epoch 87/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1868 - acc: 0.9279\n",
      "Epoch 88/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1782 - acc: 0.9300\n",
      "Epoch 89/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1775 - acc: 0.9315: 0s - loss: 0.1743 - acc: 0.933\n",
      "Epoch 90/100\n",
      "4800/4800 [==============================] - 0s 20us/step - loss: 0.1711 - acc: 0.9365\n",
      "Epoch 91/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1706 - acc: 0.9360\n",
      "Epoch 92/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1741 - acc: 0.9340\n",
      "Epoch 93/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1722 - acc: 0.9333\n",
      "Epoch 94/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1685 - acc: 0.9350\n",
      "Epoch 95/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1691 - acc: 0.9340\n",
      "Epoch 96/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1653 - acc: 0.9365\n",
      "Epoch 97/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1612 - acc: 0.9379\n",
      "Epoch 98/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1618 - acc: 0.9371\n",
      "Epoch 99/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1694 - acc: 0.9319\n",
      "Epoch 100/100\n",
      "4800/4800 [==============================] - 0s 19us/step - loss: 0.1648 - acc: 0.9342\n",
      "1200/1200 [==============================] - 0s 90us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "def build_classifier():\n",
    "    classifier=Sequential()\n",
    "    input_layer=Dense(units=40,kernel_initializer='uniform',activation='relu',input_dim=30) #adding hidden layers\n",
    "    classifier.add(input_layer)\n",
    "    for count in range(3):\n",
    "        hidden_layer=Dense(units=40,kernel_initializer='uniform',activation='relu')\n",
    "        classifier.add(hidden_layer)\n",
    "    output_layer=Dense(units=1,kernel_initializer='uniform',activation='sigmoid')# adding output layer\n",
    "    classifier.add(output_layer)\n",
    "    classifier.layers\n",
    "    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier=KerasClassifier(build_classifier,batch_size=50,epochs=100)\n",
    "accuracies=cross_val_score(estimator=classifier,X=X_train,y=Y_train,cv=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82833333, 0.81999999, 0.81833334, 0.82833333, 0.79583332])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.81666612625122"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=accuracies.mean()\n",
    "100*mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 505us/step - loss: 0.6932 - acc: 0.4935\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 541us/step - loss: 0.6932 - acc: 0.4860\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 519us/step - loss: 0.6927 - acc: 0.5190\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 503us/step - loss: 0.6932 - acc: 0.5067\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 514us/step - loss: 0.6931 - acc: 0.5019\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 490us/step - loss: 0.6931 - acc: 0.5004\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 496us/step - loss: 0.6932 - acc: 0.4938\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 502us/step - loss: 0.6932 - acc: 0.4973\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 508us/step - loss: 0.6932 - acc: 0.4912\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 2s 511us/step - loss: 0.6932 - acc: 0.4931\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 542us/step - loss: 0.6704 - acc: 0.6179\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 546us/step - loss: 0.6352 - acc: 0.6450\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 546us/step - loss: 0.6458 - acc: 0.6573\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 557us/step - loss: 0.6932 - acc: 0.4890\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 568us/step - loss: 0.6727 - acc: 0.6090\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 556us/step - loss: 0.6351 - acc: 0.6573\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 566us/step - loss: 0.6681 - acc: 0.6710\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 570us/step - loss: 0.6710 - acc: 0.6950\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 576us/step - loss: 0.6429 - acc: 0.7150\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 575us/step - loss: 0.6527 - acc: 0.6946\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 620us/step - loss: 0.5933 - acc: 0.7125\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 616us/step - loss: 0.6146 - acc: 0.6837\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 625us/step - loss: 0.5827 - acc: 0.7185\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 633us/step - loss: 0.5867 - acc: 0.6873\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 642us/step - loss: 0.5950 - acc: 0.7052\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 643us/step - loss: 0.5608 - acc: 0.7638\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 681us/step - loss: 0.5908 - acc: 0.6992\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 659us/step - loss: 0.5621 - acc: 0.7608\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 657us/step - loss: 0.5768 - acc: 0.7144\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 662us/step - loss: 0.5835 - acc: 0.7352\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 679us/step - loss: 0.5857 - acc: 0.6808\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 686us/step - loss: 0.5701 - acc: 0.6954\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 694us/step - loss: 0.5665 - acc: 0.6973\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 701us/step - loss: 0.5598 - acc: 0.6985\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 705us/step - loss: 0.5601 - acc: 0.7240\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 714us/step - loss: 0.5595 - acc: 0.7265\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 718us/step - loss: 0.5280 - acc: 0.7512\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 723us/step - loss: 0.5240 - acc: 0.7675\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 3s 728us/step - loss: 0.5369 - acc: 0.7615\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 731us/step - loss: 0.5280 - acc: 0.7483\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 750us/step - loss: 0.6932 - acc: 0.5023\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 769us/step - loss: 0.6932 - acc: 0.4877\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 776us/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 790us/step - loss: 0.6932 - acc: 0.4981\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 796us/step - loss: 0.6932 - acc: 0.5012\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 815us/step - loss: 0.6932 - acc: 0.5019\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 817us/step - loss: 0.6932 - acc: 0.4948\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 806us/step - loss: 0.6932 - acc: 0.4931\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 942us/step - loss: 0.6931 - acc: 0.4935\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 899us/step - loss: 0.6932 - acc: 0.5004\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 863us/step - loss: 0.6932 - acc: 0.4992\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 844us/step - loss: 0.6932 - acc: 0.4998\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 856us/step - loss: 0.6932 - acc: 0.4875\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 871us/step - loss: 0.6932 - acc: 0.4992\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 884us/step - loss: 0.6932 - acc: 0.4988\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 875us/step - loss: 0.6440 - acc: 0.6081\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 880us/step - loss: 0.6932 - acc: 0.5012\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 894us/step - loss: 0.6932 - acc: 0.4925\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 898us/step - loss: 0.6886 - acc: 0.5973\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 909us/step - loss: 0.6785 - acc: 0.6510\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 4s 932us/step - loss: 0.6576 - acc: 0.5687\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 940us/step - loss: 0.6687 - acc: 0.5763\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 952us/step - loss: 0.6931 - acc: 0.5038\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 966us/step - loss: 0.6931 - acc: 0.5031\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 975us/step - loss: 0.6514 - acc: 0.5819\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 964us/step - loss: 0.6062 - acc: 0.6892\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 978us/step - loss: 0.6236 - acc: 0.6477\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 977us/step - loss: 0.6063 - acc: 0.6796\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 985us/step - loss: 0.6134 - acc: 0.6877\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 992us/step - loss: 0.6248 - acc: 0.6608\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.6571 - acc: 0.5898\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.6058 - acc: 0.6385\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.5937 - acc: 0.6723\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.6041 - acc: 0.6437\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.6249 - acc: 0.6238\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.5666 - acc: 0.7173\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.5361 - acc: 0.7452\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.5888 - acc: 0.7065\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.5462 - acc: 0.7425\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.5601 - acc: 0.7165\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.5040\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.4985\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6931 - acc: 0.5040\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.4883\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.4935\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.6932 - acc: 0.5006\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.6932 - acc: 0.5023\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.6932 - acc: 0.4988\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 9s 2ms/step - loss: 0.6932 - acc: 0.4969\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.6932 - acc: 0.5010\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.6932 - acc: 0.5025\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.6932 - acc: 0.4948\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.6932 - acc: 0.4933\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4994\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4998\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4963\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4863\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4877\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.5035\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.5050\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6932 - acc: 0.4933\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.6570 - acc: 0.5881\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 9s 2ms/step - loss: 0.6907 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6734 - acc: 0.5873\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 9s 2ms/step - loss: 0.6166 - acc: 0.6685\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6447 - acc: 0.6179\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 11s 2ms/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6932 - acc: 0.4981\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6932 - acc: 0.4952\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6932 - acc: 0.5021\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.5888 - acc: 0.6908\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6216 - acc: 0.6617\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6932 - acc: 0.4979\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 11s 2ms/step - loss: 0.5699 - acc: 0.6965\n",
      "Epoch 1/1\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.6366 - acc: 0.6292\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 11s 2ms/step - loss: 0.5776 - acc: 0.7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x0000025C8DAA37B8>,\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'node_count': [5, 15, 25, 35], 'layer_count': [3, 4, 5], 'opt': ['adam', 'rmsprop']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def neural_network(node_count,layer_count,opt):\n",
    "    classifier=Sequential()\n",
    "    input_layer=Dense(units=node_count,input_dim=30, activation='relu',kernel_initializer='uniform')\n",
    "    classifier.add(input_layer)\n",
    "    for count in range(layer_count):\n",
    "        hidden_layer=Dense(units=node_count,activation='relu',kernel_initializer='uniform')\n",
    "        classifier.add(hidden_layer)\n",
    "    output_layer=Dense(units=1,activation='sigmoid',kernel_initializer='uniform')\n",
    "    classifier.add(output_layer)\n",
    "    classifier.compile(optimizer=opt,metrics=['accuracy'],loss='binary_crossentropy')\n",
    "    return classifier\n",
    "\n",
    "#importing the KerasClassifier inorder to facilitate hyperparameter tuning:\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "neural_classifier=KerasClassifier(build_fn=neural_network)\n",
    "parameters={'node_count':[5,15,25,35],'layer_count':[3,4,5],'opt':['adam','rmsprop']}\n",
    "\n",
    "#importing gridsearchcv:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_object=GridSearchCV(estimator=neural_classifier,cv=5,scoring='accuracy',param_grid=parameters)\n",
    "grid_object.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_count': 3, 'node_count': 25, 'opt': 'adam'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_object.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 10s 2ms/step - loss: 0.5929 - acc: 0.7415\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 1s 199us/step - loss: 0.4075 - acc: 0.8258\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 1s 197us/step - loss: 0.3900 - acc: 0.8333\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 1s 212us/step - loss: 0.3786 - acc: 0.8392\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.3695 - acc: 0.8438\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 1s 235us/step - loss: 0.3620 - acc: 0.8492\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 1s 231us/step - loss: 0.3547 - acc: 0.8482\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.3481 - acc: 0.8562\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.3421 - acc: 0.8585\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.3384 - acc: 0.8648\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.3359 - acc: 0.8615\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 1s 216us/step - loss: 0.3335 - acc: 0.8642\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 1s 213us/step - loss: 0.3297 - acc: 0.8673\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 1s 214us/step - loss: 0.3272 - acc: 0.8668\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.3239 - acc: 0.8683\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 1s 210us/step - loss: 0.3216 - acc: 0.8728\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 1s 204us/step - loss: 0.3198 - acc: 0.8702\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 1s 206us/step - loss: 0.3163 - acc: 0.8720\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 1s 210us/step - loss: 0.3145 - acc: 0.8738\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 1s 207us/step - loss: 0.3107 - acc: 0.8767\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 1s 205us/step - loss: 0.3107 - acc: 0.8747\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.3095 - acc: 0.8783\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 1s 212us/step - loss: 0.3066 - acc: 0.8778\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 1s 208us/step - loss: 0.3042 - acc: 0.8782\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 1s 208us/step - loss: 0.3019 - acc: 0.8787\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 1s 214us/step - loss: 0.3023 - acc: 0.8775\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 1s 217us/step - loss: 0.3013 - acc: 0.8812\n",
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.3005 - acc: 0.8803\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 1s 216us/step - loss: 0.2998 - acc: 0.8795\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 1s 210us/step - loss: 0.2964 - acc: 0.8823\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 1s 212us/step - loss: 0.2962 - acc: 0.8822\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 1s 223us/step - loss: 0.2954 - acc: 0.8823\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 1s 244us/step - loss: 0.2939 - acc: 0.8837\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 1s 217us/step - loss: 0.2934 - acc: 0.8838\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.2931 - acc: 0.8832\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 1s 213us/step - loss: 0.2913 - acc: 0.8865\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.2889 - acc: 0.8830\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 1s 222us/step - loss: 0.2904 - acc: 0.8850\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 1s 213us/step - loss: 0.2890 - acc: 0.8837\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.2887 - acc: 0.8853\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 1s 223us/step - loss: 0.2840 - acc: 0.8862\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.2865 - acc: 0.8885\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 1s 219us/step - loss: 0.2871 - acc: 0.8872\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 1s 217us/step - loss: 0.2855 - acc: 0.8875\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 1s 219us/step - loss: 0.2847 - acc: 0.8867\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.2842 - acc: 0.8870\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 1s 219us/step - loss: 0.2828 - acc: 0.8863\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.2818 - acc: 0.8888\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 1s 227us/step - loss: 0.2807 - acc: 0.8868\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.2799 - acc: 0.8870\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 1s 226us/step - loss: 0.2794 - acc: 0.8888\n",
      "Epoch 52/100\n",
      "6000/6000 [==============================] - 1s 223us/step - loss: 0.2786 - acc: 0.8877\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 1s 230us/step - loss: 0.2771 - acc: 0.8882 1s - loss: \n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 1s 226us/step - loss: 0.2764 - acc: 0.8910\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.2775 - acc: 0.8895\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.2747 - acc: 0.8933\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 1s 226us/step - loss: 0.2759 - acc: 0.8907\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.2762 - acc: 0.8892\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 1s 227us/step - loss: 0.2725 - acc: 0.8923\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.2742 - acc: 0.8922\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 1s 227us/step - loss: 0.2724 - acc: 0.8923\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 1s 231us/step - loss: 0.2725 - acc: 0.8910\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.2714 - acc: 0.8945\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 1s 233us/step - loss: 0.2702 - acc: 0.8910\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.2705 - acc: 0.8935\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 1s 230us/step - loss: 0.2693 - acc: 0.8928\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 1s 223us/step - loss: 0.2697 - acc: 0.8947\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 1s 237us/step - loss: 0.2686 - acc: 0.8937\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 1s 236us/step - loss: 0.2673 - acc: 0.8950\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - 1s 227us/step - loss: 0.2667 - acc: 0.8945\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 1s 228us/step - loss: 0.2654 - acc: 0.8952\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 1s 230us/step - loss: 0.2656 - acc: 0.8947\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 1s 228us/step - loss: 0.2655 - acc: 0.8960\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 1s 231us/step - loss: 0.2645 - acc: 0.8935\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 1s 229us/step - loss: 0.2647 - acc: 0.8945\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 1s 230us/step - loss: 0.2633 - acc: 0.8945\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 2s 253us/step - loss: 0.2648 - acc: 0.8932\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 1s 237us/step - loss: 0.2627 - acc: 0.8950\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 1s 234us/step - loss: 0.2642 - acc: 0.8978\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 1s 225us/step - loss: 0.2615 - acc: 0.8970\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 1s 227us/step - loss: 0.2601 - acc: 0.8950\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 1s 224us/step - loss: 0.2621 - acc: 0.8962\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 1s 224us/step - loss: 0.2608 - acc: 0.8965\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 1s 217us/step - loss: 0.2597 - acc: 0.8993\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 1s 222us/step - loss: 0.2599 - acc: 0.8957\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 1s 207us/step - loss: 0.2585 - acc: 0.8998\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 1s 224us/step - loss: 0.2580 - acc: 0.8978\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - 1s 211us/step - loss: 0.2588 - acc: 0.8970\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 1s 210us/step - loss: 0.2555 - acc: 0.9007\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.2591 - acc: 0.8953\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 1s 211us/step - loss: 0.2561 - acc: 0.8980\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.2555 - acc: 0.8998\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 1s 220us/step - loss: 0.2571 - acc: 0.9003\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.2560 - acc: 0.8982\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 1s 224us/step - loss: 0.2543 - acc: 0.8985\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 1s 218us/step - loss: 0.2561 - acc: 0.8993\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 1s 222us/step - loss: 0.2540 - acc: 0.9027\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 1s 214us/step - loss: 0.2520 - acc: 0.8987\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.2512 - acc: 0.9007\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 1s 223us/step - loss: 0.2527 - acc: 0.8995\n"
     ]
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "input_layer=Dense(units=node_count,input_dim=30, activation='relu',kernel_initializer='uniform')\n",
    "classifier.add(input_layer)\n",
    "for count in range(layer_count):\n",
    "    hidden_layer=Dense(units=node_count,activation='relu',kernel_initializer='uniform')\n",
    "    classifier.add(hidden_layer)\n",
    "output_layer=Dense(units=1,activation='sigmoid',kernel_initializer='uniform')\n",
    "classifier.add(output_layer)\n",
    "classifier.compile(optimizer=opt,metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30824  6098]\n",
      " [  383  1906]]\n",
      "83.47147484124353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(100*accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
